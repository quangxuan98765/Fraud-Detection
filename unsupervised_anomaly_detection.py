"""
Unsupervised Anomaly Detection Pipeline Using Neo4j Graph Data Science
Author: Xuan Quang
Date: May 14, 2025

Pipeline n√†y th·ª±c hi·ªán ph√°t hi·ªán gian l·∫≠n kh√¥ng gi√°m s√°t s·ª≠ d·ª•ng c√°c thu·∫≠t to√°n h·ªçc m√°y d·ª±a tr√™n ƒë·ªì th·ªã
ƒë·ªÉ t√≠nh to√°n ƒëi·ªÉm b·∫•t th∆∞·ªùng (anomaly score) cho m·ªói giao d·ªãch.
"""

from neo4j import GraphDatabase
import pandas as pd
import numpy as np
import time
import json
import os
import logging

# Disable Neo4j driver's INFO and WARNING logs
logging.getLogger("neo4j").setLevel(logging.ERROR)

# C·∫•u h√¨nh k·∫øt n·ªëi Neo4j
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "12345678"  # Thay ƒë·ªïi password n·∫øu c·∫ßn

class UnsupervisedFraudDetection:
    def __init__(self, uri, user, password):
        """Kh·ªüi t·∫°o k·∫øt n·ªëi Neo4j v√† c·∫•u h√¨nh ph√°t hi·ªán gian l·∫≠n."""
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        self.weights = {
            'degScore': 0.20,
            'prScore': 0.20,
            'normCommunitySize': 0.15,  # Inverted in calculation (1-normCommunitySize)
            'simScore': 0.10,
            'btwScore': 0.10,
            'hubScore': 0.05,
            'authScore': 0.05,
            'coreScore': 0.05,
            'triCount': 0.05,
            'cycleCount': 0.05,
            'tempBurst': 0.05
        }
        self.percentile_cutoff = 0.95  # Ng∆∞·ª°ng ph√¢n v·ªã 95% m·∫∑c ƒë·ªãnh
        
    def close(self):
        """ƒê√≥ng k·∫øt n·ªëi Neo4j."""
        self.driver.close()
        
    def run_query(self, query, params=None):
        """Ch·∫°y truy v·∫•n Cypher tr√™n Neo4j v√† tr·∫£ v·ªÅ t·∫•t c·∫£ c√°c b·∫£n ghi."""
        with self.driver.session() as session:
            try:
                if params:
                    result = session.run(query, params)
                else:
                    result = session.run(query)
                    
                # Check if the result has records without consuming them
                has_records = result.peek() is not None
                
                if not has_records:
                    # For queries that don't return data (CREATE, SET, DELETE, ...)
                    return None
                else:
                    # For queries that return data, collect them first to avoid consumption issues
                    data = result.data()
                    if len(data) == 0:
                        return None
                        
                    # If we expect to use single() later (like in calculate_community_sizes)
                    # Return the first record directly
                    return data[0]
            except Exception as e:
                print(f"Query error: {str(e)}")
                raise e

    def examine_data(self):
        """
        Ki·ªÉm tra v√† x√°c th·ª±c d·ªØ li·ªáu tr∆∞·ªõc khi th·ª±c hi·ªán ph√¢n t√≠ch.
        - Ki·ªÉm tra s·ª± ph√¢n b·ªë c·ªßa ground truth data
        - Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu c·ªßa c√°c ƒë·∫∑c tr∆∞ng
        - Ki·ªÉm tra s·ª± nh·∫•t qu√°n v√† ƒë·∫ßy ƒë·ªß c·ªßa d·ªØ li·ªáu
        """
        print("üîç ƒêang ki·ªÉm tra d·ªØ li·ªáu...")
        
        # 1. Ki·ªÉm tra s·ª± t·ªìn t·∫°i v√† ph√¢n b·ªë c·ªßa ground_truth_fraud
        ground_truth_query = """
        MATCH ()-[tx:SENT]->()
        WITH 
            COUNT(tx) AS total,
            SUM(CASE WHEN tx.ground_truth_fraud IS NOT NULL THEN 1 ELSE 0 END) AS has_ground_truth,
            SUM(CASE WHEN tx.ground_truth_fraud = true THEN 1 ELSE 0 END) AS fraud_cases
        RETURN 
            total, 
            has_ground_truth, 
            fraud_cases,
            toFloat(has_ground_truth) / total AS coverage_ratio,
            CASE WHEN has_ground_truth > 0 
                THEN toFloat(fraud_cases) / has_ground_truth 
                ELSE 0 
            END AS fraud_ratio
        """
        
        result = self.run_query(ground_truth_query)
        
        if result:
            total = result["total"]
            has_ground_truth = result["has_ground_truth"]
            fraud_cases = result["fraud_cases"]
            coverage_ratio = result["coverage_ratio"]
            fraud_ratio = result["fraud_ratio"]
            
            print(f"\nüìä Ph√¢n t√≠ch ground truth data:")
            print(f"  ‚Ä¢ T·ªïng s·ªë giao d·ªãch: {total}")
            print(f"  ‚Ä¢ S·ªë giao d·ªãch c√≥ ground truth: {has_ground_truth} ({coverage_ratio*100:.2f}%)")
            print(f"  ‚Ä¢ S·ªë giao d·ªãch gian l·∫≠n: {fraud_cases} ({fraud_ratio*100:.2f}%)")
            
            # C·∫£nh b√°o n·∫øu t·ª∑ l·ªá ph·ªß qu√° th·∫•p
            if coverage_ratio < 0.5:
                print(f"  ‚ö†Ô∏è C·∫£nh b√°o: Ch·ªâ {coverage_ratio*100:.2f}% giao d·ªãch c√≥ ground truth data.")
            
            # C·∫£nh b√°o n·∫øu t·ª∑ l·ªá gian l·∫≠n qu√° cao ho·∫∑c qu√° th·∫•p
            if fraud_ratio < 0.001:
                print(f"  ‚ö†Ô∏è C·∫£nh b√°o: T·ª∑ l·ªá gian l·∫≠n qu√° th·∫•p ({fraud_ratio*100:.4f}%).")
            elif fraud_ratio > 0.2:
                print(f"  ‚ö†Ô∏è C·∫£nh b√°o: T·ª∑ l·ªá gian l·∫≠n qu√° cao ({fraud_ratio*100:.2f}%).")
        else:
            print("  ‚ùå Kh√¥ng th·ªÉ truy v·∫•n th√¥ng tin ground truth data.")
        
        # 2. Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu c·ªßa ground_truth_fraud
        type_check_query = """
        MATCH ()-[tx:SENT]->()
        WHERE tx.ground_truth_fraud IS NOT NULL
        RETURN 
            CASE 
                WHEN toString(tx.ground_truth_fraud) IN ['true', 'false'] THEN 'String'
                WHEN toString(tx.ground_truth_fraud) IN ['0', '1'] THEN 'String'
                WHEN tx.ground_truth_fraud IN [true, false] THEN 'Boolean'
                ELSE 'Unknown'
            END AS data_type,
            COUNT(*) as count
        """
        
        try:
            # Run this query directly since it uses APOC
            with self.driver.session() as session:
                type_check_result = session.run(type_check_query).data()
                
            if type_check_result:
                print("\nüìä Ki·ªÉu d·ªØ li·ªáu c·ªßa ground_truth_fraud:")
                for record in type_check_result:
                    print(f"  ‚Ä¢ {record['data_type']}: {record['count']} giao d·ªãch")
                    
                    # C·∫£nh b√°o n·∫øu c√≥ ki·ªÉu d·ªØ li·ªáu kh√¥ng ph·∫£i boolean
                    if record['data_type'] != 'Boolean' and record['data_type'] != 'boolean':
                        print(f"  ‚ö†Ô∏è C·∫£nh b√°o: ground_truth_fraud c√≥ ki·ªÉu d·ªØ li·ªáu {record['data_type']} thay v√¨ Boolean.")
        except Exception as e:
            print(f"  ‚ùå Kh√¥ng th·ªÉ ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu: {str(e)}")
            print("  üîÑ S·∫Ω ti·∫øp t·ª•c v·ªõi gi·∫£ ƒë·ªãnh ground_truth_fraud l√† String ho·∫∑c Boolean.")
        
        # 3. Ki·ªÉm tra ph√¢n ph·ªëi c·ªßa anomaly score (n·∫øu c√≥)
        score_distribution_query = """
        MATCH ()-[tx:SENT]->()
        WHERE tx.anomaly_score IS NOT NULL
        WITH 
            MIN(tx.anomaly_score) AS min_score,
            MAX(tx.anomaly_score) AS max_score,
            AVG(tx.anomaly_score) AS avg_score,
            STDEV(tx.anomaly_score) AS std_score,
            percentileCont(tx.anomaly_score, 0.5) AS median_score,
            percentileCont(tx.anomaly_score, 0.95) AS p95_score,
            percentileCont(tx.anomaly_score, 0.99) AS p99_score,
            COUNT(*) AS count
        RETURN min_score, max_score, avg_score, std_score, median_score, p95_score, p99_score, count
        """
        
        score_result = self.run_query(score_distribution_query)
        
        if score_result and score_result["count"] > 0:
            print("\nüìä Ph√¢n ph·ªëi c·ªßa anomaly score:")
            print(f"  ‚Ä¢ S·ªë giao d·ªãch c√≥ anomaly score: {score_result['count']}")
            print(f"  ‚Ä¢ Gi√° tr·ªã nh·ªè nh·∫•t: {score_result['min_score']:.6f}")
            print(f"  ‚Ä¢ Gi√° tr·ªã l·ªõn nh·∫•t: {score_result['max_score']:.6f}")
            print(f"  ‚Ä¢ Gi√° tr·ªã trung b√¨nh: {score_result['avg_score']:.6f}")
            print(f"  ‚Ä¢ ƒê·ªô l·ªách chu·∫©n: {score_result['std_score']:.6f}")
            print(f"  ‚Ä¢ Gi√° tr·ªã trung v·ªã: {score_result['median_score']:.6f}")
            print(f"  ‚Ä¢ Ph√¢n v·ªã 95%: {score_result['p95_score']:.6f}")
            print(f"  ‚Ä¢ Ph√¢n v·ªã 99%: {score_result['p99_score']:.6f}")
            
            # C·∫£nh b√°o n·∫øu ph√¢n ph·ªëi kh√¥ng ƒë·ªÅu
            if score_result['std_score'] < 0.001:
                print("  ‚ö†Ô∏è C·∫£nh b√°o: Ph√¢n ph·ªëi anomaly score qu√° t·∫≠p trung (ƒë·ªô l·ªách chu·∫©n th·∫•p).")
        
        print("‚úÖ ƒê√£ ho√†n th√†nh ki·ªÉm tra d·ªØ li·ªáu.")
        
    def prepare_ground_truth(self):
        """Map isFraud t·ª´ CSV sang ground_truth_fraud ƒë·ªÉ h·ªó tr·ª£ ƒë√°nh gi√°."""
        print("üîÑ ƒêang chu·∫©n b·ªã d·ªØ li·ªáu ground truth...")
        
        # Ki·ªÉm tra xem isFraud c√≥ t·ªìn t·∫°i trong SENT relationships kh√¥ng
        check_query = """
        MATCH ()-[r:SENT]->()
        RETURN 
            COUNT(r) AS total,
            SUM(CASE WHEN r.isFraud IS NOT NULL THEN 1 ELSE 0 END) AS has_is_fraud
        """
        result = self.run_query(check_query)
        
        if result and result["has_is_fraud"] > 0:
            print(f"  ‚Ä¢ T√¨m th·∫•y {result['has_is_fraud']} giao d·ªãch c√≥ tr∆∞·ªùng isFraud")
            
            # Map t·ª´ isFraud sang ground_truth_fraud
            map_query = """
            MATCH ()-[r:SENT]->()
            WHERE r.isFraud IS NOT NULL AND r.ground_truth_fraud IS NULL
            SET r.ground_truth_fraud = CASE 
                WHEN r.isFraud = 1 OR r.isFraud = true OR r.isFraud = '1' THEN true 
                ELSE false 
            END
            RETURN COUNT(*) AS mapped
            """
            
            map_result = self.run_query(map_query)
            if map_result:
                print(f"  ‚úÖ ƒê√£ map {map_result['mapped']} giao d·ªãch t·ª´ isFraud sang ground_truth_fraud")
        else:
            print("  ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y tr∆∞·ªùng isFraud trong d·ªØ li·ªáu SENT relationships")
        
        # Ki·ªÉm tra k·∫øt qu·∫£
        final_check = """
        MATCH ()-[r:SENT]->()
        RETURN 
            COUNT(r) AS total,
            SUM(CASE WHEN r.ground_truth_fraud IS NOT NULL THEN 1 ELSE 0 END) AS has_ground_truth,
            SUM(CASE WHEN r.ground_truth_fraud = true THEN 1 ELSE 0 END) AS fraud_cases
        """
        
        final_result = self.run_query(final_check)
        if final_result:
            total = final_result["total"]
            has_ground_truth = final_result["has_ground_truth"]
            fraud_cases = final_result["fraud_cases"]
            
            print(f"\nüìä Th√¥ng tin ground truth sau khi chu·∫©n b·ªã:")
            print(f"  ‚Ä¢ T·ªïng s·ªë giao d·ªãch: {total}")
            print(f"  ‚Ä¢ S·ªë giao d·ªãch c√≥ ground truth: {has_ground_truth} ({has_ground_truth/total*100:.2f}%)")
            print(f"  ‚Ä¢ S·ªë giao d·ªãch gian l·∫≠n: {fraud_cases} ({fraud_cases/has_ground_truth*100:.2f}% trong s·ªë c√≥ nh√£n)")
        
    def create_graph_projections(self):
        """T·∫°o c√°c graph projection d√πng cho c√°c thu·∫≠t to√°n GDS."""
        print("üîÑ ƒêang t·∫°o c√°c graph projection...")
        
        # T·∫°o timestamp ƒë·ªÉ ƒë·∫£m b·∫£o t√™n graph l√† duy nh·∫•t
        timestamp = int(time.time())
        self.main_graph_name = f'main-graph-{timestamp}'
        self.similarity_graph_name = f'account-similarity-{timestamp}'
        self.temporal_graph_name = f'temporal-graph-{timestamp}'
        
        # 1. Graph projection cho c√°c Account v√† m·ªëi quan h·ªá SENT
        main_projection = f"""
        CALL gds.graph.project(
            '{self.main_graph_name}',
            'Account',
            {{
                SENT: {{
                    type: 'SENT',
                    orientation: 'NATURAL',
                    properties: {{
                        weight: {{
                            property: 'amount',
                            defaultValue: 0.0,
                            aggregation: 'NONE'
                        }}
                    }}
                }}
            }}
        )
        """
        self.run_query(main_projection)
        
        # 2. Graph projection cho account similarity
        filtered_similarity_projection = f"""
        CALL gds.graph.project.cypher(
            '{self.similarity_graph_name}',
            'MATCH (a:Account) 
            WHERE EXISTS((a)-[:SENT]->())  // ƒê·∫£m b·∫£o node c√≥ g·ª≠i transaction
            RETURN id(a) AS id, labels(a) AS labels',
            'MATCH (a:Account)-[:SENT]->(tx:Transaction)-[:RECEIVED]->(b:Account)
            RETURN id(a) AS source, id(b) AS target, "TRANSFER" AS type',
            {{
                validateRelationships: false
            }}
        ) YIELD graphName AS filteredGraphName
        RETURN filteredGraphName
        """
        self.run_query(filtered_similarity_projection)
        
        # 3. Graph projection cho temporal analysis
        temporal_projection = f"""
        CALL gds.graph.project(
            '{self.temporal_graph_name}',
            'Account',
            {{
                SENT: {{
                    type: 'SENT',
                    orientation: 'NATURAL',
                    properties: {{
                        weight: {{
                            property: 'step',
                            defaultValue: 0,
                            aggregation: 'NONE'
                        }}
                    }}
                }}
            }}
        )
        """
        self.run_query(temporal_projection)
        
        print("‚úÖ ƒê√£ t·∫°o xong c√°c graph projection.")

    def extract_temporal_features(self):
        """Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng th·ªùi gian (temporal features) ƒë·ªÉ ph√°t hi·ªán m·∫´u b·∫•t th∆∞·ªùng."""
        print("üîÑ ƒêang tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng th·ªùi gian...")
        
        # 1. T√≠nh t·ªëc ƒë·ªô giao d·ªãch (giao d·ªãch/gi·ªù) trong c·ª≠a s·ªï th·ªùi gian
        transaction_velocity_query = """
        MATCH (from:Account)-[tx:SENT]->()
        WITH from, tx.step as step
        ORDER BY from, step
        WITH from, collect(step) AS steps
        WITH from, steps, 
            size(steps) AS transaction_count,
            CASE WHEN size(steps) <= 1 THEN 0 ELSE toFloat(last(steps) - head(steps)) END AS time_span
        WITH from, transaction_count, 
            CASE WHEN time_span = 0 THEN 0 ELSE transaction_count / (time_span + 1) END AS velocity
        SET from.txVelocity = velocity
        """
        self.run_query(transaction_velocity_query)
        
        # 2. Ph√°t hi·ªán s·ª± thay ƒë·ªïi ƒë·ªôt ng·ªôt trong s·ªë ti·ªÅn giao d·ªãch (s·ª≠a l·∫°i ƒë·ªÉ ho·∫°t ƒë·ªông ƒë√∫ng)
        simple_volatility_query = """
        MATCH (from:Account)-[tx:SENT]->()
        WITH from, tx
        ORDER BY from, tx.step
        WITH from, collect(tx.amount) as amount_list
        WITH from, amount_list,
            CASE WHEN size(amount_list) <= 1 THEN 0 
                ELSE (
                    // T√≠nh range t·ª´ng ph·∫ßn t·ª≠ m·ªôt
                    REDUCE(max_val = 0, x IN amount_list | 
                    CASE WHEN x > max_val THEN x ELSE max_val END
                    ) - 
                    REDUCE(min_val = toFloat(9999999999), x IN amount_list | 
                    CASE WHEN x < min_val AND x IS NOT NULL THEN x ELSE min_val END
                    )
                ) 
            END AS amount_range,
            CASE WHEN size(amount_list) = 0 THEN 0 
                ELSE REDUCE(sum = 0, x IN amount_list | sum + x) / size(amount_list) 
            END AS avg_amount
        SET from.amountVolatility = CASE WHEN avg_amount = 0 THEN 0 ELSE amount_range / avg_amount END,
            from.maxAmountRatio = CASE WHEN avg_amount = 0 THEN 0 
                                    ELSE REDUCE(max_val = 0, x IN amount_list | 
                                            CASE WHEN x > max_val THEN x ELSE max_val END
                                        ) / avg_amount
                                END
        """
        self.run_query(simple_volatility_query)
        
        # 3. Ph√°t hi·ªán burst (nhi·ªÅu giao d·ªãch trong th·ªùi gian ng·∫Øn) - (ƒê√£ ho·∫°t ƒë·ªông)
        burst_detection_query = """
        MATCH (from:Account)-[tx:SENT]->()
        WITH from, tx.step as step
        ORDER BY from, step
        WITH from, collect(step) AS steps
        UNWIND range(0, size(steps)-2) AS i
        WITH from, steps[i+1] - steps[i] AS time_diff
        WITH from, collect(time_diff) AS time_diffs
        WITH from, time_diffs,
            CASE WHEN size(time_diffs) = 0 THEN 0 
                ELSE size([t IN time_diffs WHERE t <= 3]) / toFloat(size(time_diffs)) 
            END AS burst_ratio
        SET from.tempBurst = burst_ratio
        """
        self.run_query(burst_detection_query)
        
        # 4. Th·ªùi gian trung b√¨nh v√† ƒë·ªô l·ªách chu·∫©n - (ƒê√£ ho·∫°t ƒë·ªông)
        time_patterns_query = """
        MATCH (from:Account)-[tx:SENT]->()
        WITH from, tx.step as step
        ORDER BY from, step
        WITH from, collect(step) AS steps
        UNWIND range(0, size(steps)-2) AS i
        WITH from, steps[i+1] - steps[i] AS time_diff
        WITH from, avg(time_diff) AS avg_time_between_tx,
            stDev(time_diff) AS std_time_between_tx
        SET from.avgTimeBetweenTx = avg_time_between_tx,
            from.stdTimeBetweenTx = CASE WHEN avg_time_between_tx = 0 THEN 0 
                                        ELSE std_time_between_tx / avg_time_between_tx 
                                    END
        """
        self.run_query(time_patterns_query)
        
        # C·∫≠p nh·∫≠t tr·ªçng s·ªë
        self.weights['txVelocity'] = 0.05
        self.weights['amountVolatility'] = 0.07
        self.weights['tempBurst'] = 0.08
        self.weights['maxAmountRatio'] = 0.05
        self.weights['stdTimeBetweenTx'] = 0.05
        
        print("‚úÖ ƒê√£ tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng th·ªùi gian.")
        
    def run_algorithms(self):
        """Ch·∫°y t·∫•t c·∫£ c√°c thu·∫≠t to√°n GDS ƒë·ªÉ t√≠nh to√°n c√°c ƒë·∫∑c tr∆∞ng."""
        print("üîÑ ƒêang ch·∫°y c√°c thu·∫≠t to√°n ph√¢n t√≠ch ƒë·ªì th·ªã...")
        
        # 1. Degree Centrality
        print("  - ƒêang ch·∫°y Degree Centrality...")
        degree_query = f"""
        CALL gds.degree.write(
            '{self.main_graph_name}',
            {{
                writeProperty: 'degScore',
                relationshipWeightProperty: 'weight'  // Thay 'amount' b·∫±ng 'weight'
            }}
        )
        """
        self.run_query(degree_query)
        
        # 2. PageRank
        print("  - ƒêang ch·∫°y PageRank...")
        pagerank_query = f"""
        CALL gds.pageRank.write(
            '{self.main_graph_name}',
            {{
                writeProperty: 'prScore',
                relationshipWeightProperty: 'weight',  // Thay 'amount' b·∫±ng 'weight'
                maxIterations: 20,
                dampingFactor: 0.85
            }}
        )
        """
        self.run_query(pagerank_query)
        
        # 3. Louvain Community Detection
        print("  - ƒêang ch·∫°y Louvain Community Detection...")
        community_query = f"""
        CALL gds.louvain.write(
            '{self.main_graph_name}',
            {{
                writeProperty: 'communityId',
                relationshipWeightProperty: 'weight',  // Thay 'amount' b·∫±ng 'weight'
                includeIntermediateCommunities: false,  // Added comma here
                tolerance: 0.0001,    // TƒÉng tolerance ƒë·ªÉ h·ªôi t·ª• nhanh h∆°n
                maxIterations: 10,    // Gi·ªõi h·∫°n s·ªë l·∫ßn l·∫∑p
                concurrency: 4        // S·ª≠ d·ª•ng ƒëa lu·ªìng
            }}
        )
        """
        self.run_query(community_query)

        print("  - ƒê√£ ch·∫°y Louvain Community Detection. ƒêang t√≠nh to√°n k√≠ch th∆∞·ªõc c·ªông ƒë·ªìng...")

        # T√≠nh to√°n v√† normalize community size
        community_size_query = """
        MATCH (n)
        WHERE n.communityId IS NOT NULL
        WITH n.communityId AS communityId, COUNT(*) AS size
        WHERE size >= 3  // Filter small communities with WHERE instead of HAVING
        MATCH (m)
        WHERE m.communityId = communityId
        SET m.communitySize = size

        WITH MIN(size) AS minSize, MAX(size) AS maxSize
        MATCH (n)
        WHERE n.communitySize IS NOT NULL
        SET n.normCommunitySize = 
            CASE 
                WHEN (maxSize - minSize) = 0 THEN 0
                ELSE (n.communitySize - minSize) / (maxSize - minSize)
            END
        """
        self.run_query(community_size_query)
        
        # 4. Node Similarity (Jaccard) - ch·ªâ ch·∫°y cho c√°c Account
        print("  - ƒêang ch·∫°y Node Similarity (Jaccard)...")
        similarity_query = f"""
        CALL gds.nodeSimilarity.write(
            '{self.similarity_graph_name}',
            {{
                writeProperty: 'simScore',
                writeRelationshipType: 'SIMILAR',
                similarityCutoff: 0.2,
                topK: 5,
                concurrency: 4
            }}
        )
        """
        try:
            self.run_query(similarity_query)
        except Exception as e:
            print(f"L·ªói khi ch·∫°y Node Similarity: {e}")
            # S·ª≠ d·ª•ng c√°ch thay th·∫ø: Stream m·ªôt l∆∞·ª£ng nh·ªè k·∫øt qu·∫£ v√† ghi v√†o ƒë·ªì th·ªã
            fallback_similarity_query = f"""
            CALL gds.nodeSimilarity.stream(
                '{self.similarity_graph_name}',
                {{
                    similarityCutoff: 0.2,
                    topK: 3,
                    concurrency: 4
                }}
            )
            YIELD node1, node2, similarity
            WITH gds.util.asNode(node1) AS source, gds.util.asNode(node2) AS target, similarity
            LIMIT 50000  // Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng k·∫øt qu·∫£
            SET source.simScore = CASE 
                WHEN source.simScore IS NULL OR similarity > source.simScore 
                THEN similarity ELSE source.simScore END
            RETURN COUNT(*) as relationshipsProcessed
            """
            self.run_query(fallback_similarity_query)

        # G√°n simScore m·∫∑c ƒë·ªãnh = 0 cho c√°c node ch∆∞a c√≥ score
        set_default_sim_query = """
        MATCH (n)
        WHERE n.simScore IS NULL
        SET n.simScore = 0.0
        """
        self.run_query(set_default_sim_query)
        
        # 5. Betweenness Centrality
        print("  - ƒêang ch·∫°y Betweenness Centrality...")
        betweenness_query = f"""
        CALL gds.betweenness.write(
            '{self.main_graph_name}',
            {{
                writeProperty: 'btwScore'
            }}
        )
        """
        self.run_query(betweenness_query)
        
        # 6. HITS (Hub and Authority Scores)
        print("  - ƒêang ch·∫°y HITS algorithm...")
        hits_query = f"""
        CALL gds.alpha.hits.write(
            '{self.main_graph_name}',
            {{
                writeProperty: '',
                hitsIterations: 20,  // Changed from 'iterations' to 'hitsIterations'
                authProperty: 'authScore',
                hubProperty: 'hubScore' 
            }}
        )
        """
        self.run_query(hits_query)
        
       # 7. K-Core Decomposition
        print("  - ƒêang ch·∫°y K-Core Decomposition...")
        # Create a specific undirected graph for K-Core
        kcore_graph_name = f'{self.main_graph_name}-undirected'
        kcore_projection_query = f"""
        CALL gds.graph.project(
            '{kcore_graph_name}',
            'Account',
            {{
                SENT: {{
                    type: 'SENT',
                    orientation: 'UNDIRECTED',
                    properties: {{
                        weight: {{
                            property: 'amount',  // Changed from 'weight' to 'amount'
                            defaultValue: 0.0,
                            aggregation: 'NONE'
                        }}
                    }}
                }}
            }}
        )
        """
        self.run_query(kcore_projection_query)

        # Then run K-Core on the undirected graph
        kcore_query = f"""
        CALL gds.kcore.write(
            '{kcore_graph_name}',
            {{
                writeProperty: 'coreScore'
            }}
        )
        """
        self.run_query(kcore_query)

        # Clean up the temporary graph
        kcore_cleanup_query = f"""
        CALL gds.graph.drop('{kcore_graph_name}', false)
        """
        self.run_query(kcore_cleanup_query)
        
        # 8. Clustering Coefficient (Triangle Count)
        print("  - ƒêang ch·∫°y Triangle Count...")
        # Create a specific undirected graph for Triangle Count (or reuse the K-Core graph)
        triangle_graph_name = f'{self.main_graph_name}-undirected-tri'
        triangle_projection_query = f"""
        CALL gds.graph.project(
            '{triangle_graph_name}',
            'Account',
            {{
                SENT: {{
                    type: 'SENT',
                    orientation: 'UNDIRECTED'
                }}
            }}
        )
        """
        self.run_query(triangle_projection_query)

        # Run Triangle Count on the undirected graph
        triangle_query = f"""
        CALL gds.triangleCount.write(
            '{triangle_graph_name}',
            {{
                writeProperty: 'triCount'
            }}
        )
        """
        self.run_query(triangle_query)

        # Clean up the temporary graph
        triangle_cleanup_query = f"""
        CALL gds.graph.drop('{triangle_graph_name}', false)
        """
        self.run_query(triangle_cleanup_query)
        # G√°n triCount m·∫∑c ƒë·ªãnh = 0 cho c√°c node ch∆∞a c√≥ score
        set_default_tri_query = """
        MATCH (n)
        WHERE n.triCount IS NULL
        SET n.triCount = 0
        """
        self.run_query(set_default_tri_query)
        
        # 9. Motif/Cycle Detection (s·ª≠ d·ª•ng APOC)
        print("  - ƒêang ch·∫°y Motif/Cycle Detection...")
        cycle_query = """
        MATCH (a:Account)
        OPTIONAL MATCH path = (a)-[:SENT]->(tx1:Transaction)-[:RECEIVED]->(b:Account)-[:SENT]->(tx2:Transaction)-[:RECEIVED]->(c:Account)-[:SENT]->(tx3:Transaction)-[:RECEIVED]->(a)
        WITH a, COUNT(path) AS cycleCount
        SET a.cycleCount = cycleCount
        """
        self.run_query(cycle_query)
        
        # 10. Temporal Burst Analysis
        print("  - ƒêang ch·∫°y Temporal Burst Analysis...")
        temporal_burst_query = """
        // T√≠nh s·ªë l∆∞·ª£ng giao d·ªãch trong 1 gi·ªù v√† 24 gi·ªù cho m·ªói account
        MATCH (a:Account)-[tx:SENT]->()
        WITH a, tx.step AS step
        WITH a, step, COUNT(*) AS hourlyCount
        WITH a, COLLECT({step: step, count: hourlyCount}) AS hourlyCounts

        // T√≠nh tempBurst1h (gi√° tr·ªã cao nh·∫•t trong 1 gi·ªù)
        WITH a, hourlyCounts, 
            REDUCE(max = 0, h IN hourlyCounts | CASE WHEN h.count > max THEN h.count ELSE max END) AS maxHourly
        SET a.tempBurst1h = maxHourly

        // T√≠nh tempBurst24h (s·ªë gi·ªù li√™n ti·∫øp v·ªõi √≠t nh·∫•t 1 giao d·ªãch - c√°ch ƒë∆°n gi·∫£n h√≥a)
        WITH a, hourlyCounts
        UNWIND hourlyCounts AS h
        WITH a, h.step AS step
        ORDER BY step
        // S·ª≠ d·ª•ng t·ª∑ l·ªá s·ªë gi·ªù c√≥ giao d·ªãch so v·ªõi t·ªïng kho·∫£ng th·ªùi gian l√†m proxy cho burst
        WITH a, MIN(step) AS minStep, MAX(step) AS maxStep, COUNT(DISTINCT step) AS uniqueSteps
        WITH a, 
            CASE 
                WHEN maxStep = minStep THEN 1  // N·∫øu ch·ªâ c√≥ 1 gi·ªù, burst l√† 1
                ELSE toFloat(uniqueSteps) / (maxStep - minStep + 1)  // T·ª∑ l·ªá gi·ªù ho·∫°t ƒë·ªông tr√™n t·ªïng th·ªùi gian
            END * 24 AS burstDensity  // Scale to 24 hours
        SET a.tempBurst24h = burstDensity

        // T√≠nh tempBurst t·ªïng h·ª£p (k·∫øt h·ª£p c·∫£ 1h v√† 24h)
        WITH a
        SET a.tempBurst = (a.tempBurst1h * 0.7) + (a.tempBurst24h * 0.3)
        """
        self.run_query(temporal_burst_query)
        
        # G√°n c√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh cho node n·∫øu ch∆∞a c√≥
        set_default_values_query = """
        MATCH (n)
        SET n.degScore = COALESCE(n.degScore, 0),
            n.prScore = COALESCE(n.prScore, 0),
            n.communityId = COALESCE(n.communityId, -1),
            n.normCommunitySize = COALESCE(n.normCommunitySize, 0),
            n.simScore = COALESCE(n.simScore, 0),
            n.btwScore = COALESCE(n.btwScore, 0),
            n.authScore = COALESCE(n.authScore, 0),
            n.hubScore = COALESCE(n.hubScore, 0),
            n.coreScore = COALESCE(n.coreScore, 0),
            n.triCount = COALESCE(n.triCount, 0),
            n.cycleCount = COALESCE(n.cycleCount, 0),
            n.tempBurst = COALESCE(n.tempBurst, 0)
        """
        self.run_query(set_default_values_query)
        
        print("‚úÖ ƒê√£ ch·∫°y xong t·∫•t c·∫£ c√°c thu·∫≠t to√°n.")
    
    def normalize_features(self):
        """Min-max normalize t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng v·ªÅ kho·∫£ng [0, 1]."""
        print("üîÑ ƒêang normalize c√°c ƒë·∫∑c tr∆∞ng...")
        
        features_to_normalize = [
            'degScore', 'prScore', 'simScore', 'btwScore', 'hubScore', 
            'authScore', 'coreScore', 'triCount', 'cycleCount', 'tempBurst',
            'txVelocity', 'amountVolatility', 'maxAmountRatio', 'stdTimeBetweenTx'
        ]
        for feature in features_to_normalize:
            normalize_query = f"""
            MATCH (n) 
            WHERE n.{feature} IS NOT NULL
            WITH MIN(n.{feature}) AS min_val, MAX(n.{feature}) AS max_val
            WHERE max_val <> min_val
            MATCH (m)
            WHERE m.{feature} IS NOT NULL
            SET m.{feature}_norm = (m.{feature} - min_val) / (max_val - min_val)
            """
            self.run_query(normalize_query)
            
            # X√≥a c√°c ƒë·∫∑c tr∆∞ng g·ªëc v√† ƒë·ªïi t√™n c√°c ƒë·∫∑c tr∆∞ng ƒë√£ normalize
            rename_query = f"""
            MATCH (n)
            WHERE n.{feature}_norm IS NOT NULL
            SET n.{feature} = n.{feature}_norm
            REMOVE n.{feature}_norm
            """
            self.run_query(rename_query)
            
            # Thi·∫øt l·∫≠p gi√° tr·ªã m·∫∑c ƒë·ªãnh cho c√°c node kh√¥ng c√≥ ƒë·∫∑c tr∆∞ng
            default_query = f"""
            MATCH (n)
            WHERE n.{feature} IS NULL
            SET n.{feature} = 0
            """
            self.run_query(default_query)
            
        print("‚úÖ ƒê√£ normalize xong t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng.")
    
    def compute_anomaly_scores(self):
        """T√≠nh ƒëi·ªÉm b·∫•t th∆∞·ªùng (anomaly score) d·ª±a tr√™n weighted sum."""
        print("üîÑ ƒêang t√≠nh to√°n anomaly score...")
        
        # T·∫°o weighted sum c·ªßa t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng ƒë√£ normalize
        weights_str = ", ".join([f"{k} * {v}" for k, v in self.weights.items() if k != 'normCommunitySize'])
        
        anomaly_score_query = """
        MATCH (a:Account)
        WITH a, 
            a.degScore AS degScore, 
            a.prScore AS prScore,
            a.simScore AS simScore,
            a.btwScore AS btwScore,
            a.hubScore AS hubScore,
            a.authScore AS authScore,
            a.coreScore AS coreScore,
            a.triCount AS triCount,
            a.cycleCount AS cycleCount,
            a.tempBurst AS tempBurst,
            a.txVelocity AS txVelocity,
            a.amountVolatility AS amountVolatility,
            a.maxAmountRatio AS maxAmountRatio,
            a.stdTimeBetweenTx AS stdTimeBetweenTx,
            a.normCommunitySize AS normCommunitySize
        
        // T√≠nh anomaly score = weighted sum c·ªßa c√°c ƒë·∫∑c tr∆∞ng
        WITH a, 
            (degScore * 0.15) + 
            (prScore * 0.15) + 
            (simScore * 0.1) + 
            (btwScore * 0.1) + 
            (hubScore * 0.05) + 
            (authScore * 0.05) + 
            (coreScore * 0.05) + 
            (triCount * 0.05) + 
            (cycleCount * 0.05) + 
            (tempBurst * 0.08) + 
            (txVelocity * 0.05) +
            (amountVolatility * 0.07) +
            (maxAmountRatio * 0.05) +
            (stdTimeBetweenTx * 0.05) +
            (0.10 * (1 - coalesce(normCommunitySize, 0))) AS score
        
        SET a.anomaly_score = score
        """
        self.run_query(anomaly_score_query)
        
        # Chuy·ªÉn anomaly score t·ª´ Account sang Transaction
        transfer_score_query = """
        MATCH (a:Account)-[r:SENT]->()
        SET r.anomaly_score = a.anomaly_score
        """
        self.run_query(transfer_score_query)
        
        print("‚úÖ ƒê√£ t√≠nh to√°n xong anomaly score.")
    
    def flag_anomalies(self):
        """ƒê√°nh d·∫•u giao d·ªãch b·∫•t th∆∞·ªùng d·ª±a tr√™n ng∆∞·ª°ng ph√¢n v·ªã (percentile)."""
        print(f"üîÑ ƒêang ƒë√°nh d·∫•u c√°c giao d·ªãch b·∫•t th∆∞·ªùng (ng∆∞·ª°ng ph√¢n v·ªã: {self.percentile_cutoff*100}%)...")
        
        # T√≠nh gi√° tr·ªã ng∆∞·ª°ng percentile
        percentile_query = f"""
        MATCH ()-[tx:SENT]->()
        WITH percentileCont(tx.anomaly_score, {self.percentile_cutoff}) AS threshold
        MATCH ()-[tx2:SENT]->()
        WHERE tx2.anomaly_score >= threshold
        SET tx2.flagged = true
        RETURN threshold, COUNT(tx2) AS flagged_count
        """
        
        # Remove the .single() call since run_query now returns the dictionary directly
        result = self.run_query(percentile_query)
        
        if result:
            threshold = result["threshold"]
            flagged_count = result["flagged_count"]
            
            # ƒê√°nh d·∫•u c√°c giao d·ªãch kh√¥ng v∆∞·ª£t ng∆∞·ª°ng l√† kh√¥ng b·∫•t th∆∞·ªùng
            default_flagged_query = """
            MATCH ()-[tx:SENT]->()
            WHERE tx.flagged IS NULL
            SET tx.flagged = false
            """
            self.run_query(default_flagged_query)
            
            print(f"‚úÖ ƒê√£ ƒë√°nh d·∫•u {flagged_count} giao d·ªãch b·∫•t th∆∞·ªùng (threshold: {threshold:.6f}).")
        else:
            print("‚ö†Ô∏è Kh√¥ng th·ªÉ t√≠nh ng∆∞·ª°ng percentile cho anomaly score.")
    
    def evaluate_performance(self):
        """ƒê√°nh gi√° hi·ªáu su·∫•t ph√°t hi·ªán b·∫•t th∆∞·ªùng d·ª±a tr√™n ground truth."""
        print("üîÑ ƒêang ƒë√°nh gi√° hi·ªáu su·∫•t ph√°t hi·ªán b·∫•t th∆∞·ªùng...")
        
        eval_query = """
        MATCH ()-[tx:SENT]->()
        WITH
            SUM(CASE WHEN tx.flagged = true AND tx.ground_truth_fraud = true THEN 1 ELSE 0 END) AS true_positives,
            SUM(CASE WHEN tx.flagged = true AND tx.ground_truth_fraud = false THEN 1 ELSE 0 END) AS false_positives,
            SUM(CASE WHEN tx.flagged = false AND tx.ground_truth_fraud = true THEN 1 ELSE 0 END) AS false_negatives,
            SUM(CASE WHEN tx.flagged = false AND tx.ground_truth_fraud = false THEN 1 ELSE 0 END) AS true_negatives,
            COUNT(*) AS total_transactions,
            SUM(CASE WHEN tx.ground_truth_fraud = true THEN 1 ELSE 0 END) AS total_fraud
        
        // Calculate precision and recall first
        WITH
            true_positives, false_positives, false_negatives, true_negatives, 
            total_transactions, total_fraud,
            CASE WHEN (true_positives + false_positives) > 0 
                THEN toFloat(true_positives) / (true_positives + false_positives) 
                ELSE 0 
            END AS precision,
            CASE WHEN (true_positives + false_negatives) > 0 
                THEN toFloat(true_positives) / (true_positives + false_negatives) 
                ELSE 0 
            END AS recall
        
        // Then use precision and recall to calculate F1 score
        RETURN 
            true_positives,
            false_positives,
            false_negatives,
            true_negatives,
            total_transactions,
            total_fraud,
            precision,
            recall,
            CASE 
                WHEN (precision + recall) > 0 
                THEN 2 * precision * recall / (precision + recall) 
                ELSE 0 
            END AS f1_score
        """
        
        result = self.run_query(eval_query)
        
        # T√≠nh c√°c metric kh√°c
        accuracy = (result["true_positives"] + result["true_negatives"]) / result["total_transactions"]
        
        # Prepare detailed metrics report
        metrics = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "model": "unsupervised_anomaly_detection",
            "parameters": {
                "weights": self.weights,
                "percentile_cutoff": self.percentile_cutoff
            },
            "metrics": {
                "true_positives": result["true_positives"],
                "false_positives": result["false_positives"],
                "false_negatives": result["false_negatives"],
                "true_negatives": result["true_negatives"],
                "total_transactions": result["total_transactions"],
                "total_fraud": result["total_fraud"],
                "precision": result["precision"],
                "recall": result["recall"],
                "f1_score": result["f1_score"],
                "accuracy": accuracy
            }
        }
        
        # L∆∞u metrics ra file
        with open('unsupervised_anomaly_detection_metrics.json', 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2)
        
        # Hi·ªÉn th·ªã metrics
        print("\nüìä K·∫øt qu·∫£ ƒë√°nh gi√° hi·ªáu su·∫•t:")
        print(f"  ‚Ä¢ T·ªïng s·ªë giao d·ªãch: {result['total_transactions']}")
        print(f"  ‚Ä¢ T·ªïng s·ªë giao d·ªãch gian l·∫≠n th·ª±c t·∫ø: {result['total_fraud']}")
        print(f"  ‚Ä¢ S·ªë giao d·ªãch b·∫•t th∆∞·ªùng ƒë∆∞·ª£c ƒë√°nh d·∫•u: {result['true_positives'] + result['false_positives']}")
        print(f"  ‚Ä¢ True Positives: {result['true_positives']}")
        print(f"  ‚Ä¢ False Positives: {result['false_positives']}")
        print(f"  ‚Ä¢ False Negatives: {result['false_negatives']}")
        print(f"  ‚Ä¢ True Negatives: {result['true_negatives']}")
        print(f"  ‚Ä¢ Precision: {result['precision']:.4f}")
        print(f"  ‚Ä¢ Recall: {result['recall']:.4f}")
        print(f"  ‚Ä¢ F1 Score: {result['f1_score']:.4f}")
        print(f"  ‚Ä¢ Accuracy: {accuracy:.4f}")
        print(f"\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o file unsupervised_anomaly_detection_metrics.json")
        
        return metrics
    
    def analyze_feature_importance(self):
        """Ph√¢n t√≠ch t·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng s·ª≠ d·ª•ng Python thay v√¨ APOC."""
        print("üîÑ ƒêang ph√¢n t√≠ch t·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng...")
        
        import numpy as np
        
        # Function to calculate correlation without APOC
        def calculate_correlation(list1, list2):
            if not list1 or not list2 or len(list1) != len(list2):
                return 0
            try:
                # Convert boolean values to integers for correlation calculation
                list1_numeric = [1 if x else 0 for x in list1]
                # Ensure numeric values for list2
                list2_numeric = [float(x) if x is not None else 0 for x in list2]
                
                # If all values are identical, correlation is not defined
                if np.std(list1_numeric) == 0 or np.std(list2_numeric) == 0:
                    return 0
                    
                return np.corrcoef(list1_numeric, list2_numeric)[0, 1]
            except Exception as e:
                print(f"Error calculating correlation: {e}")
                return 0
        
        # T√≠nh t∆∞∆°ng quan gi·ªØa c√°c ƒë·∫∑c tr∆∞ng v√† ground truth fraud
        features = list(self.weights.keys())
        correlations = {}
        
        for feature in features:
            # L·∫§Y ƒê·∫∂C TR∆ØNG T·ª™ NODE ACCOUNT, K·∫æT H·ª¢P V·ªöI GROUND_TRUTH_FRAUD T·ª™ RELATIONSHIP
            data_query = f"""
            MATCH (a:Account)-[tx:SENT]->()
            WHERE tx.ground_truth_fraud IS NOT NULL AND a.{feature} IS NOT NULL
            RETURN tx.ground_truth_fraud AS fraud, a.{feature} AS feature_value
            """
            
            try:
                # Get all records
                result = []
                with self.driver.session() as session:
                    result = session.run(data_query).data()
                
                if result:
                    # Extract lists for correlation
                    fraud_values = [record['fraud'] for record in result]
                    feature_values = [record['feature_value'] for record in result]
                    
                    # Calculate correlation
                    correlation = calculate_correlation(fraud_values, feature_values)
                    correlations[feature] = correlation
                    print(f"  ‚úÖ Ph√¢n t√≠ch {feature}: {len(result)} giao d·ªãch, t∆∞∆°ng quan = {correlation:.4f}")
                else:
                    print(f"  ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu cho {feature}")
                    correlations[feature] = 0
            except Exception as e:
                print(f"  ‚ö†Ô∏è Kh√¥ng th·ªÉ t√≠nh t∆∞∆°ng quan cho {feature}: {str(e)}")
                correlations[feature] = 0
        
        # S·∫Øp x·∫øp c√°c ƒë·∫∑c tr∆∞ng theo ƒë·ªô quan tr·ªçng (gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa t∆∞∆°ng quan)
        sorted_features = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
        
        print("\nüìä T·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng:")
        for feature, correlation in sorted_features:
            print(f"  ‚Ä¢ {feature}: {correlation:.4f}")
        
        return sorted_features
    
    def delete_graph_projections(self):
        """X√≥a c√°c graph projections ƒë√£ t·∫°o."""
        print("üîÑ ƒêang x√≥a c√°c graph projections...")
        
        # X√≥a c√°c graph projections c·ª• th·ªÉ
        try:
            self.run_query(f"CALL gds.graph.drop('{self.main_graph_name}', false)")
            self.run_query(f"CALL gds.graph.drop('{self.similarity_graph_name}', false)")
            self.run_query(f"CALL gds.graph.drop('{self.temporal_graph_name}', false)")
            self.run_query(f"CALL gds.graph.drop('{self.main_graph_name}-undirected', false)")
            self.run_query(f"CALL gds.graph.drop('{self.main_graph_name}-undirected-tri', false)")
            print("‚úÖ ƒê√£ x√≥a t·∫•t c·∫£ c√°c graph projections.")
        except Exception as e:
            print(f"‚ö†Ô∏è L∆∞u √Ω khi x√≥a graph: {str(e)}")

    def cleanup_properties_and_relationships(self):
        """X√≥a t·∫•t c·∫£ c√°c thu·ªôc t√≠nh ƒë∆∞·ª£c th√™m v√†o trong qu√° tr√¨nh ph√¢n t√≠ch ƒë·ªÉ tr√°nh ƒë·∫ßy database."""
        print("üîÑ ƒêang d·ªçn d·∫πp c√°c thu·ªôc t√≠nh ph√¢n t√≠ch...")
        
        # Danh s√°ch c√°c thu·ªôc t√≠nh ƒë∆∞·ª£c th√™m v√†o trong qu√° tr√¨nh ph√¢n t√≠ch
        added_properties = [
            'degScore', 'prScore', 'communityId', 'communitySize', 'normCommunitySize',
            'simScore', 'btwScore', 'hubScore', 'authScore', 'coreScore', 'triCount',
            'cycleCount', 'tempBurst', 'tempBurst1h', 'tempBurst24h', 'anomaly_score', 'flagged'
        ]
        
        # X√≥a thu·ªôc t√≠nh tr√™n t·∫•t c·∫£ c√°c node
        properties_to_remove = ", ".join([f"n.{prop}" for prop in added_properties])
        cleanup_query = f"""
        MATCH (n)
        REMOVE {properties_to_remove}
        """

        relationship_cleanup_query = """
        MATCH ()-[r:SENT]->()
        REMOVE r.anomaly_score, r.flagged
        """
        
        try:
            self.run_query(cleanup_query)
            self.run_query(relationship_cleanup_query)
            print(f"‚úÖ ƒê√£ x√≥a {len(added_properties)} thu·ªôc t√≠nh ph√¢n t√≠ch kh·ªèi database.")
        except Exception as e:
            print(f"‚ùå L·ªói khi d·ªçn d·∫πp thu·ªôc t√≠nh: {str(e)}")
            
        # X√≥a c√°c m·ªëi quan h·ªá SIMILAR (t·ª´ Node Similarity)
        try:
            self.run_query("MATCH ()-[r:SIMILAR]-() DELETE r")
            print("‚úÖ ƒê√£ x√≥a c√°c m·ªëi quan h·ªá SIMILAR.")
        except Exception as e:
            print(f"‚ùå L·ªói khi x√≥a quan h·ªá SIMILAR: {str(e)}")
    
    def run_pipeline(self, percentile_cutoff=None):
        """
        Ch·∫°y to√†n b·ªô pipeline ph√°t hi·ªán b·∫•t th∆∞·ªùng.
        
        Args:
            percentile_cutoff: Ng∆∞·ª°ng ph√¢n v·ªã ƒë·ªÉ ƒë√°nh d·∫•u giao d·ªãch b·∫•t th∆∞·ªùng (m·∫∑c ƒë·ªãnh: 0.95)
        
        Returns:
            dict: Metrics ƒë√°nh gi√° hi·ªáu su·∫•t
        """
        if percentile_cutoff is not None:
            self.percentile_cutoff = percentile_cutoff
            
        start_time = time.time()
        
        print("=" * 50)
        print("üöÄ B·∫Øt ƒë·∫ßu ch·∫°y pipeline ph√°t hi·ªán b·∫•t th∆∞·ªùng kh√¥ng gi√°m s√°t")
        print("=" * 50)
        
        # 1. Chu·∫©n b·ªã d·ªØ li·ªáu ground truth
        self.prepare_ground_truth()

        # 2. Ki·ªÉm tra v√† s·ª≠a l·ªói d·ªØ li·ªáu
        self.examine_data()
        
        # 3. T·∫°o graph projections
        self.create_graph_projections()

        # 4. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng th·ªùi gian (TH√äM B∆Ø·ªöC M·ªöI)
        self.extract_temporal_features()
        
        # 5. Ch·∫°y c√°c thu·∫≠t to√°n Graph Data Science
        self.run_algorithms()
        
        # 6. Normalize c√°c ƒë·∫∑c tr∆∞ng
        self.normalize_features()
        
        # 7. T√≠nh to√°n anomaly score
        self.compute_anomaly_scores()
        
        # 8. ƒê√°nh d·∫•u c√°c giao d·ªãch b·∫•t th∆∞·ªùng
        self.flag_anomalies()
        
        # 9. ƒê√°nh gi√° hi·ªáu su·∫•t
        metrics = self.evaluate_performance()
        
        # 10. Ph√¢n t√≠ch t·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng
        feature_importances = self.analyze_feature_importance()

        # 11. X√≥a c√°c graph projections
        self.delete_graph_projections()

        # 12. D·ªçn d·∫πp c√°c thu·ªôc t√≠nh v√† m·ªëi quan h·ªá kh√¥ng c·∫ßn thi·∫øt
        self.cleanup_properties_and_relationships()

        end_time = time.time()
        execution_time = end_time - start_time
        
        print("\n‚è±Ô∏è Th·ªùi gian th·ª±c thi: {:.2f} gi√¢y".format(execution_time))
        print("=" * 50)
        print("‚úÖ Ho√†n th√†nh pipeline ph√°t hi·ªán b·∫•t th∆∞·ªùng kh√¥ng gi√°m s√°t")
        print("=" * 50)
        
        return metrics

# Ch·∫°y pipeline
if __name__ == "__main__":    
    # Kh·ªüi t·∫°o v√† ch·∫°y pipeline
    fraud_detector = UnsupervisedFraudDetection(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)
    
    try:
        # Ch·∫°y v·ªõi ng∆∞·ª°ng ph√¢n v·ªã t·ªëi ∆∞u ƒë√£ x√°c ƒë·ªãnh
        percentile = 0.97
        print(f"\nüìä Ch·∫°y pipeline v·ªõi ng∆∞·ª°ng ph√¢n v·ªã {percentile*100}%")
        metrics = fraud_detector.run_pipeline(percentile)

        print("\n" + "=" * 50)
        print(f"üèÜ K·∫øt qu·∫£ v·ªõi ng∆∞·ª°ng ph√¢n v·ªã {percentile*100}%")
        print(f"   F1 Score: {metrics['metrics']['f1_score']:.4f}")
        print(f"   Precision: {metrics['metrics']['precision']:.4f}")
        print(f"   Recall: {metrics['metrics']['recall']:.4f}")
        print("=" * 50)

        # Th·ª≠ v·ªõi nhi·ªÅu ng∆∞·ª°ng percentile kh√°c nhau ƒë·ªÉ t√¨m ng∆∞·ª°ng t·ªëi ∆∞u
        # percentiles = [0.93, 0.95, 0.97]
        # results = []
        
        # for p in percentiles:
        #     print(f"\nüìä Th·ª≠ nghi·ªám v·ªõi ng∆∞·ª°ng ph√¢n v·ªã {p*100}%")
        #     metrics = fraud_detector.run_pipeline(p)
        #     results.append({
        #         "percentile": p,
        #         "metrics": metrics
        #     })

        # # T√¨m ng∆∞·ª°ng t·ªët nh·∫•t d·ª±a tr√™n F1 score
        # best_result = max(results, key=lambda x: x["metrics"]["metrics"]["f1_score"])
        
        # print("\n" + "=" * 50)
        # print(f"üèÜ Ng∆∞·ª°ng ph√¢n v·ªã t·ªëi ∆∞u: {best_result['percentile']*100}%")
        # print(f"   F1 Score: {best_result['metrics']['metrics']['f1_score']:.4f}")
        # print(f"   Precision: {best_result['metrics']['metrics']['precision']:.4f}")
        # print(f"   Recall: {best_result['metrics']['metrics']['recall']:.4f}")
        # print("=" * 50)
        
        # # L∆∞u k·∫øt qu·∫£ so s√°nh
        # comparison = {
        #     "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        #     "percentile_comparison": [
        #         {
        #             "percentile": r["percentile"],
        #             "f1_score": r["metrics"]["metrics"]["f1_score"],
        #             "precision": r["metrics"]["metrics"]["precision"],
        #             "recall": r["metrics"]["metrics"]["recall"]
        #         } for r in results
        #     ],
        #     "best_percentile": best_result["percentile"]
        # }
        
        # with open('percentile_comparison.json', 'w', encoding='utf-8') as f:
        #     json.dump(comparison, f, indent=2)
        
        # print("‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ so s√°nh c√°c ng∆∞·ª°ng ph√¢n v·ªã v√†o file percentile_comparison.json")
    
    finally:
        # ƒê√≥ng k·∫øt n·ªëi Neo4j
        fraud_detector.close()
